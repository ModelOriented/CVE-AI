---
title: "Validation report 001: MIDI-to-score Conversion Model"
collection: publications
permalink: /publication/2024-01-10-MIDI
excerpt: 'This project aims to explore advancements in automatically transcribing performance MIDI streams into musical scores, focusing on the paper "Performance MIDI-to-score Conversion by Neural Beat Tracking" by Liu et al. (2022). The study reveals artifacts in score generation models, particularly with velocity contributions and time signature robustness, and highlights the need for more tailored explainable AI (XAI) methods for symbolic music data to enhance model interpretability.'
date: 2024-01-10
venue: 'Explainable Machine Learning 2023/2024 course'
paperurl: 'https://modeloriented.github.io/CVE-AI/files/2023_MIDI.pdf'
citation: 'Mateusz Szymanski. (2024). &quot;Performance MIDI To Score Automatic Transcription.&quot; <i>Github: ModelOriented/CVE-AI</i>.'
tags:
  - midi
  - artifacts
---

The goal of this project is to explore and understand the latest advancements in the automatic transcription of performance MIDI streams into musical scores, speciâ€€cally within the broader scope of Audio Music Transcription (AMT). We analyze the recent paper Performance MIDI-to-score Conversion by Neural Beat Tracking (Liu et al., 2022), which comprises two main components: rhythm quantization and score generation based on a Convolutional Recurrent Neural Network (CRNN) architecture. 

We investigate the model's behavior using sequence perturbations and a LIME-like approach.

<hr/>

We were able to discover certain artifacts of the score generation models, especially when it comes to velocity contribution to hand part assignment. The time signature model is not fully robust to alterations that should not have affect the output.

Unfortunately, the proposed methods have drawbacks and are not fully justified. Current XAI methods do not work well with symbolic music data in general. Developing
more tailored and adaptable XAI methods for musical applications could contribute to improved model interpretability. 
One of the challenge would be to find a resonable and interpretable embedding of the pitch space that encodes musical features.

<hr/>

Link to original publication with a model: [midi_quantisation_paper_ismir_2022_0.pdf](https://www.turing.ac.uk/sites/default/files/2022-09/midi_quantisation_paper_ismir_2022_0.pdf)

